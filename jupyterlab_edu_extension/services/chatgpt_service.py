"""
ChatGPT æ•´åˆæœå‹™ï¼ˆå„ªåŒ–ç‰ˆï¼‰

æä¾›ç¨‹å¼ç¢¼åˆ†æèˆ‡å°è©±åŠŸèƒ½ï¼Œä½¿ç”¨ OpenAI APIã€‚
æ”¯æ´ä¸²æµå›æ‡‰ã€å¿«å–æ©Ÿåˆ¶ã€‚
"""

import asyncio
import hashlib
import json
from collections import OrderedDict
from typing import Any, AsyncIterator

from openai import AsyncOpenAI

from ..config import get_settings


class LRUCache:
    """ç°¡æ˜“ LRU å¿«å–"""
    
    def __init__(self, max_size: int = 100):
        self.cache: OrderedDict[str, str] = OrderedDict()
        self.max_size = max_size
    
    def get(self, key: str) -> str | None:
        if key in self.cache:
            # ç§»åˆ°æœ€å¾Œï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def set(self, key: str, value: str) -> None:
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€èˆŠçš„é …ç›®
                self.cache.popitem(last=False)
        self.cache[key] = value
    
    @staticmethod
    def make_key(*args: Any) -> str:
        """å¾åƒæ•¸å»ºç«‹å¿«å–éµ"""
        content = json.dumps(args, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content.encode()).hexdigest()


class ChatGPTService:
    """ChatGPT æœå‹™é¡åˆ¥ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    
    # é¡åˆ¥ç´šåˆ¥çš„å¿«å–ï¼ˆæ‰€æœ‰å¯¦ä¾‹å…±äº«ï¼‰
    _error_cache = LRUCache(max_size=200)
    _code_cache = LRUCache(max_size=100)

    def __init__(self):
        """åˆå§‹åŒ– ChatGPT æœå‹™"""
        settings = get_settings()
        self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.model = settings.openai_model

    async def analyze_error(self, code: str, error: str, use_cache: bool = True) -> str:
        """åˆ†æç¨‹å¼éŒ¯èª¤ï¼ˆæ”¯æ´å¿«å–ï¼‰

        Args:
            code: ç¨‹å¼ç¢¼å…§å®¹
            error: éŒ¯èª¤è¨Šæ¯
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–

        Returns:
            ç¹é«”ä¸­æ–‡çš„éŒ¯èª¤åˆ†æèˆ‡å»ºè­°
        """
        # æª¢æŸ¥å¿«å–
        if use_cache:
            cache_key = self._error_cache.make_key(code.strip(), error.strip())
            cached = self._error_cache.get(cache_key)
            if cached:
                return cached

        system_prompt = """ä½ æ˜¯ä¸€ä½è¦ªåˆ‡çš„ç¨‹å¼æ•™å­¸åŠ©æ•™ï¼Œå°ˆé–€å¹«åŠ©åˆå­¸è€…ç†è§£ç¨‹å¼éŒ¯èª¤ã€‚

ä½ çš„ä»»å‹™æ˜¯ï¼š
1. ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹éŒ¯èª¤è¨Šæ¯çš„å«ç¾©
2. æŒ‡å‡ºç¨‹å¼ç¢¼ä¸­å°è‡´éŒ¯èª¤çš„å…·é«”ä½ç½®
3. æä¾›ä¿®æ­£å»ºè­°
4. å¦‚æœé©åˆï¼Œçµ¦äºˆå­¸ç¿’ç›¸é—œæ¦‚å¿µçš„æç¤º

è«‹ä½¿ç”¨ç°¡æ½”ã€æ˜“æ‡‚çš„èªè¨€ï¼Œé¿å…éæ–¼å°ˆæ¥­çš„è¡“èªã€‚
å›æ‡‰æ ¼å¼ï¼š
## ğŸ” éŒ¯èª¤èªªæ˜
ï¼ˆéŒ¯èª¤é¡å‹èˆ‡åŸå› èªªæ˜ï¼‰

## ğŸ“ å•é¡Œä½ç½®
ï¼ˆæŒ‡å‡ºç¨‹å¼ç¢¼ä¸­çš„å•é¡Œï¼‰

## âœ… ä¿®æ­£å»ºè­°
ï¼ˆå…·é«”çš„ä¿®æ­£æ–¹å¼ï¼‰

## ğŸ’¡ å­¸ç¿’æç¤º
ï¼ˆç›¸é—œæ¦‚å¿µæˆ–å¸¸è¦‹é™·é˜±ï¼‰
"""

        user_message = f"""è«‹åˆ†æä»¥ä¸‹ç¨‹å¼ç¢¼çš„éŒ¯èª¤ï¼š

**ç¨‹å¼ç¢¼ï¼š**
```python
{code}
```

**éŒ¯èª¤è¨Šæ¯ï¼š**
```
{error}
```
"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
            )

            result = response.choices[0].message.content or "ç„¡æ³•åˆ†ææ­¤éŒ¯èª¤"
            
            # å„²å­˜åˆ°å¿«å–
            if use_cache:
                self._error_cache.set(cache_key, result)
            
            return result

        except Exception as e:
            return f"åˆ†æéŒ¯èª¤æ™‚ç™¼ç”Ÿå•é¡Œï¼š{e!s}"

    async def analyze_code(self, code: str, context: str | None = None) -> str:
        """åˆ†æç¨‹å¼ç¢¼

        Args:
            code: ç¨‹å¼ç¢¼å…§å®¹
            context: é¡å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯é¸ï¼‰

        Returns:
            ç¹é«”ä¸­æ–‡çš„ç¨‹å¼ç¢¼åˆ†æ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ç¨‹å¼æ•™å­¸åŠ©æ•™ï¼Œå¹«åŠ©å­¸ç”Ÿç†è§£ç¨‹å¼ç¢¼çš„é‹ä½œæ–¹å¼ã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼š
1. é€æ­¥è§£é‡‹ç¨‹å¼ç¢¼çš„åŠŸèƒ½
2. èªªæ˜é—œéµèªæ³•èˆ‡æ¦‚å¿µ
3. å¦‚æœæœ‰æ”¹é€²ç©ºé–“ï¼Œæä¾›å»ºè­°

ä½¿ç”¨ç°¡æ½”ã€æ˜“æ‡‚çš„èªè¨€ã€‚"""

        user_message = f"è«‹è§£é‡‹é€™æ®µç¨‹å¼ç¢¼ï¼š\n\n```python\n{code}\n```"
        if context:
            user_message += f"\n\né¡å¤–èƒŒæ™¯è³‡è¨Šï¼š{context}"

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
            )

            return response.choices[0].message.content or "ç„¡æ³•åˆ†ææ­¤ç¨‹å¼ç¢¼"

        except Exception as e:
            return f"åˆ†æç¨‹å¼ç¢¼æ™‚ç™¼ç”Ÿå•é¡Œï¼š{e!s}"

    async def chat(
        self,
        message: str,
        notebook_context: dict[str, Any] | None = None,
    ) -> str:
        """èˆ‡ ChatGPT é€²è¡Œå°è©±ï¼ˆéä¸²æµï¼‰

        Args:
            message: ç”¨æˆ¶è¨Šæ¯
            notebook_context: Notebook ä¸Šä¸‹æ–‡

        Returns:
            ChatGPT å›æ‡‰
        """
        messages = self._build_chat_messages(message, notebook_context)

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
            )

            return response.choices[0].message.content or "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•å›æ‡‰é€™å€‹å•é¡Œã€‚"

        except Exception as e:
            return f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e!s}"

    async def chat_stream(
        self,
        message: str,
        notebook_context: dict[str, Any] | None = None,
    ) -> AsyncIterator[str]:
        """èˆ‡ ChatGPT é€²è¡Œä¸²æµå°è©±

        Args:
            message: ç”¨æˆ¶è¨Šæ¯
            notebook_context: Notebook ä¸Šä¸‹æ–‡

        Yields:
            ä¸²æµçš„æ–‡å­—ç‰‡æ®µ
        """
        messages = self._build_chat_messages(message, notebook_context)

        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
            )

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            yield f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e!s}"

    def _build_chat_messages(
        self,
        message: str,
        notebook_context: dict[str, Any] | None = None,
    ) -> list[dict[str, str]]:
        """å»ºæ§‹èŠå¤©è¨Šæ¯åˆ—è¡¨"""
        system_prompt = """ä½ æ˜¯ä¸€ä½å‹å–„çš„ç¨‹å¼æ•™å­¸åŠ©æ•™ï¼Œæ­£åœ¨å”åŠ©å­¸ç”Ÿå­¸ç¿’ Python ç¨‹å¼è¨­è¨ˆã€‚

ä½ å¯ä»¥å­˜å–å­¸ç”Ÿç›®å‰æ­£åœ¨ç·¨è¼¯çš„ Jupyter Notebook å…§å®¹ã€‚è«‹æ ¹æ“šé€™äº›ä¸Šä¸‹æ–‡ä¾†å›ç­”å•é¡Œã€‚

å›æ‡‰è¦å‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
2. èªªæ˜è¦æ¸…æ¥šã€æ˜“æ‡‚
3. é©æ™‚æä¾›ç¨‹å¼ç¢¼ç¯„ä¾‹
4. é¼“å‹µå­¸ç”Ÿæ€è€ƒï¼Œè€Œä¸æ˜¯ç›´æ¥çµ¦ç­”æ¡ˆ
5. å¦‚æœå­¸ç”Ÿå•çš„å•é¡Œèˆ‡ Notebook å…§å®¹ç„¡é—œï¼Œä¹Ÿå¯ä»¥æ­£å¸¸å›ç­”
"""

        # å»ºæ§‹ä¸Šä¸‹æ–‡è¨Šæ¯
        context_message = ""
        if notebook_context and notebook_context.get("cells"):
            cells = notebook_context["cells"]
            current_index = notebook_context.get("current_cell_index", 0)

            context_parts = ["ç›®å‰ Notebook çš„å…§å®¹ï¼š\n"]
            for i, cell in enumerate(cells[:20]):  # é™åˆ¶æœ€å¤š 20 å€‹ cells
                cell_type = cell.get("type", "code")
                content = cell.get("content", "")[:500]  # é™åˆ¶æ¯å€‹ cell å…§å®¹é•·åº¦

                marker = "ğŸ‘‰ " if i == current_index else ""
                context_parts.append(f"{marker}[{cell_type.upper()} Cell {i + 1}]\n{content}\n")

            context_message = "\n".join(context_parts)

        messages = [{"role": "system", "content": system_prompt}]

        if context_message:
            messages.append({
                "role": "system",
                "content": f"ä»¥ä¸‹æ˜¯å­¸ç”Ÿç›®å‰çš„ Notebook å…§å®¹ä¾›ä½ åƒè€ƒï¼š\n\n{context_message}",
            })

        messages.append({"role": "user", "content": message})
        return messages
